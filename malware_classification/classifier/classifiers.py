# AI Usage Statement
# Tools Used: GitHub Copilot

# Usage: Code completion for transforming csv into a dict with pandas
# Verification: The dict has the same length as if making it with csv

# Usage: Brainstorming solutions for memory usage issues
# Verification: I didn't ask for specific implementations, only ideas. I decided to do chuncking, and it explained how to go about it. 

# Prohibited Use Compliance: Confirmed

import zipfile
import torch
import numpy as np
import pandas as pd

class Storage:
    '''
    Class for storing data
    feature_vector_file - zip file with feature vector files
    families_file - csv file with family information
    benign - benign samples to use for training. clamps the value between 0 and 123453
    malware - malware samples to use for training. clamps the value between 0 and 5560
    '''
    def __init__(self, feature_vectors_file: str, families_file: str, benign_max = 5560, malware_max = 5560):
        self.feature_vectors_file = feature_vectors_file
        self.families_file = families_file
        # first, get minimum between maximum allowed and the parameter. then, take the maximum between 0 and the result.
        self.benign_max = max(0, min(benign_max, 123453))
        self.malware_max = max(0, min(malware_max, 5560)) 

        self.possible_features = self._possible_features()
        
        self.families = self._families()
        self.possible_families = set(self.families.values())

        self.samples = self._samples()

    '''
    def save(self, path):
        with open(path, 'wb') as f:
            pickle.dump(self.__dict__, f)

    @classmethod
    def load(cls, path):
        storage = cls.__new__(cls)  # Create instance without calling __init__
        with open(path, 'rb') as f:
            storage.__dict__ = pickle.load(f)
        return storage
    '''
        
    def _samples(self):
        '''
        Returns a dictionary {sample_hash:vector} (str:dict)
        '''
        samples = {}

        with zipfile.ZipFile(self.feature_vectors_file, 'r') as zip:
            m = b = 0

            for feature_file in zip.namelist():

                # check if we have enough samples
                if (b >= self.benign_max) and (m >= self.malware_max):
                    break
                
                # ignore the zip itself
                if feature_file == 'feature_vectors/':
                    continue

                with zip.open(feature_file) as ff:
                    content = ff.read()
                    features = content.strip().split(b'\n')

                hash = feature_file[len('/feature_vectors/')-1:] # strip the path from the hash

                if hash in self.families.keys():
                    family = self.families[hash]
                    string_label = 'malware'
                    label = 1 # 1 for malware

                    if m < self.malware_max:
                        sample = Sample(hash, self.possible_features) 
                        samples[hash] = sample
                        sample.add_features(features)

                        sample.string_label = string_label
                        sample.label = label
                        sample.family = family

                        m += 1
                else:
                    string_label = 'benign'
                    label = -1 # -1 for benign

                    if b < self.benign_max: 
                        sample = Sample(hash, self.possible_features) 
                        samples[hash] = sample
                        sample.add_features(features)

                        sample.string_label = string_label
                        sample.label = label
                        sample.family = family

                        b += 1

        print(f'Benign Samples: {b}')
        print(f'Malware Samples: {m}')
        
        return samples

    def _possible_features(self) -> set:
        '''
        Returns a set of all possible features
        '''
        possible_features = set()

        with zipfile.ZipFile(self.feature_vectors_file, 'r') as zip:
            for feature_file in zip.namelist(): # iterate over all file names
                with zip.open(feature_file) as ff: # read the files
                    content = ff.read()
                    features = content.strip().split(b'\n') # get each line into a list

                    for feature in features:
                        if feature:
                            possible_features.add(feature.strip())
        
        print(f'Possible features: {len(possible_features)}')
        return possible_features
    
    def _families(self):
        '''
        returns a dict {sample_hash:family} (str:str)
        '''
        df = pd.read_csv(self.families_file, header=0) # use first row as a header
        return dict(zip(df['sha256'], df['family']))
    
    def _ordered_lists(self, chunk_size = 2500):
        '''
        Calculates and returns feature and label lists
        Sample features only show features for that sample
        This adds the 0s
        return feature_list [[features] for all samples], label_list [label for all samples]
        '''
        num_samples = len(self.samples)
        num_features = len(self.possible_features)

        try:
            feature_array = np.zeros((num_samples, num_features), dtype=np.uint8)
            label_array = np.zeros(num_samples, dtype=np.int8)
        except MemoryError as e:
            print(f'{e}.\n Change the sample size!')
            exit()
        
        # Convert possible_features to list once for indexing, since sets don't guarantee order
        features_list = list(self.possible_features)
        # Pre-allocate a zeros array for each sample
        feature_vector = [0] * len(features_list)
        
        # Create dict for faster feature lookup
        feature_to_idx = {feature: idx for idx, feature in enumerate(features_list)}

        samples_list = list(self.samples.values())
        
        # go over the chunks
        for start in range(0, num_samples, chunk_size):

            # clamp if greater than sample size
            end = min(start + chunk_size, num_samples)
            current_chunk_size = end - start

            # uses less memory
            feature_array = np.zeros((current_chunk_size, num_features), dtype=np.uint8)
            label_array = np.zeros(current_chunk_size, dtype=np.int8)
            
            # iterate over the chunk. 
            for idx_in_chunk, sample in enumerate(samples_list[start:end]):

                for feature in sample.features: # if feature is present in the sample

                    if feature in feature_to_idx: # if 
                        feature_array[idx_in_chunk, feature_to_idx[feature]] = 1

                label_array[idx_in_chunk] = sample.label
            
            # yield pauses the process, allowing to return to it
            yield feature_array, label_array

class Sample:
    '''
    Class for storing samples
    '''

    def __init__(self, hash, possible_features) -> None:
        self.hash = hash
        
        # only get the features that are present in the sample. Add the 0s before making the X and Y lists
        self.features = {}

        self.family = None
        self.string_label = None
        self.label = 0 # 0 for unknown
         
    def add_features(self, features: list):
        for feature in features:
            if feature:
                self.features[feature] = 1

class _Algorithm:
    '''
    Generic methods and constructors for ML algorithms
    '''

    def __init__(self, storage: Storage) -> None:
        self.storage = storage
        self.samples = storage.samples
        self.possible_features = storage.possible_features
        self.families = storage.families
        self.possible_families = storage.possible_families

    def train():
        raise NotImplementedError()
    
    def evaluate():
        raise NotImplementedError()

class SVM(_Algorithm):
    '''
    Support Vector Machine Algorithm
    Performs Binary Classification - benign samples are -1 and malware is +1
    '''
    def __init__(self, *args, **kwargs) -> None:
        super(SVM, self).__init__(*args, **kwargs)
        self.name = "Support Vector Machine"
        self.train()
        self.evaluate()

    def get_tensors(self, X: list, Y: list):
        '''
        Returns tuple - X_tensor, Y_tensor
        '''
    
        X_tensor = torch.tensor(X, dtype=torch.float32)
        Y_tensor = torch.tensor(Y, dtype=torch.float32)

        return X_tensor, Y_tensor
    
    def train(self, **kwargs):
        '''
        Train model utilizing Stochastic Gradient Descent with Hinge Loss
        loss = max(0, 1 - t * y_pred)
        '''
        
        print('Training...')
        num_features = len(self.storage.possible_features)
         
        #Set weights and bias's to 0
        self.weight = torch.zeros(num_features, dtype=torch.float32, requires_grad=True) 
        self.bias = torch.zeros(1, dtype=torch.float32, requires_grad=True)

        epochs = kwargs.get('e', 5) # Number of times model goes through files
        learning_rate = kwargs.get('learning_rate', 0.05) # Size of step model takes when adjusting weights and bias
        C = kwargs.get('c', 1.0) # Regularization parameter
        
        # the same as for epoch in epoch but with a bar
        
        for epoch in range(epochs):

            # At the start of each epoch, set the total loss to zero
            total_loss = 0.0
            total_samples = 0
            
            # Process each chunk yielded by the ordered lists generator
            for feature_array, label_array in self.storage._ordered_lists(chunk_size=2500):

                X_chunk, Y_chunk = self.get_tensors(feature_array, label_array)
                chunk_size_curr = X_chunk.shape[0]
                total_samples += chunk_size_curr

                # Process each sample in the chunk
                for i in range(chunk_size_curr):

                    x_i = X_chunk[i] # Features of the file
                    y_i = Y_chunk[i] # Label (benign or malicious)
                    
                    # Compute the first prediction: positive if likely malware and negative if likely benign
                    y_pred = torch.dot(self.weight, x_i) - self.bias
                    
                    # max(0, 1 - t * y_pred). - Hinge loss - >= to 1, prediction is correct and confident
                    hinge_loss = torch.clamp(1 - y_i * y_pred, min=0)
                    
                    # Add regularization term to stop weights from becoming too large
                    loss = hinge_loss + C * torch.norm(self.weight)**2
                    
                    # Auto calulcate changes in w and b for loss reducation
                    loss.backward()
                    
                    # Update parameters using SGD and stops pytorch from auto updating
                    # Weights added 
                    with torch.no_grad():
                        self.weight -= learning_rate * self.weight.grad
                        self.bias -= learning_rate * self.bias.grad
                        
                    # Clear out the gradients to recompute next loop
                    self.weight.grad.zero_()
                    self.bias.grad.zero_()
                    
                    # Track total loss
                    total_loss += loss.item()

            avg_loss = total_loss / total_samples if total_samples else 0
            print(f"Epoch {epoch} average loss: {avg_loss:.4f}")
                
            # Delete the chunk tensors to free memory
            del X_chunk, Y_chunk

            
        print('Training complete.')

    def evaluate(self):
        '''
        Evaluate the SVM using the training data
        '''
        pass