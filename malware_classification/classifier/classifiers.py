# AI Usage Statement
# Tools Used: GitHub Copilot

# Usage: Code completion for transforming csv into a dict with pandas
# Verification: The dict has the same length as if making it with csv

# Usage: Brainstorming solutions for memory usage issues
# Verification: I didn't ask for specific implementations, only ideas. I decided to do chuncking, and it explained how to go about it.

# Usage: Fed copilot the instructions for SVM training and asked where the code was different from them.
# Didn't ask for specific improvements, just steps that were implemented incorrectly.
# Verification: the model did not generate any new ideas or code, only cross-referenced existing text.

# Usage: Getting feature weight from tensor and correctly sorting numpy arrays.
# Verification: Revied torch and numpy docs.

# Prohibited Use Compliance: Confirmed

import zipfile
import torch
import pickle
import numpy as np
import pandas as pd
from copy import deepcopy

class Storage:
    '''
    Class for storing data.\n
    The data is first loaded in order from the file. If the sample type count is >= max, the sample is not added.\n
    The data is then split into two dicts - \n\tsamples {hash:sample} and \n\tsamples_classes {malware:[samples], benign:[samples], family: [samples]}. The malware list includes all items in the familiy lists\n
    @param:\n
    \tfeature_vector_file - zip file with feature vector files\n
    \tfamilies_file - csv file with family information\n
    \tbenign - number of benign samples to load. clamps the value between 0 and 123453\n
    \tmalware - number malware samples to load. clamp the value between 0 and 5560.\n\t
    '''
    def __init__(self, feature_vectors_file: str, families_file: str, benign_max = 5560, malware_max = 5560, **kwargs):
        self.feature_vectors_file = feature_vectors_file
        self.families_file = families_file
        # first, get minimum between maximum allowed and the parameter. then, take the maximum between 0 and the result.
        self.benign_max = max(0, min(benign_max, 123453))
        self.malware_max = max(0, min(malware_max, 5560)) 

        self.possible_features = self._possible_features()
        self.idx = {feature: idx for idx, feature in enumerate(self.possible_features)}
        
        # hold all the families available in the datafile!
        self.families = self._families()

        self.samples, self.samples_classes = self._samples()

        # get a dict of unique families to count
        # and a list of unique families
        self.possible_families_list = [sample.family for sample in self.samples.values()]

        self.possible_families_dict = {}
        for family in self.possible_families_list:
            if family not in self.possible_families_dict:
                self.possible_families_dict[family] = 1
            else:
                self.possible_families_dict[family] += 1

        self.possible_families_list = list(set(self.possible_families_list))

    def save(self, path):
        with open(path, 'wb') as f:
            pickle.dump(self.__dict__, f)

    @classmethod
    def load(cls, path):
        storage = cls.__new__(cls)  # Create instance without calling __init__
        with open(path, 'rb') as f:
            storage.__dict__ = pickle.load(f)
        return storage
        
    def _samples(self):
        '''
        Returns a dictionary {sample_hash:vector} (str:dict)
        '''
        samples = {}
        samples_classes = {'benign':[],'malware':[]} 

        with zipfile.ZipFile(self.feature_vectors_file, 'r') as zip:
            m = b = 0

            for feature_file in zip.namelist():

                # check if we have enough samples
                if (b >= self.benign_max) and (m >= self.malware_max):
                    break
                
                # ignore the zip itself
                if feature_file == 'feature_vectors/':
                    continue

                with zip.open(feature_file) as ff:
                    content = ff.read()
                    features = content.strip().split(b'\n')

                hash = feature_file[len('/feature_vectors/')-1:] # strip the path from the hash


                # TODO??? Everything below could be written so much better
                if hash in self.families.keys():
                    family = self.families[hash]
                    string_label = 'malware'
                    label = 1 # 1 for malware

                    if m < self.malware_max:
                        # create the sample and add it to dicts
                        sample = Sample(hash, self.possible_features)
                        samples[hash] = sample
                        samples_classes[string_label].append(sample)
                        if family not in samples_classes:
                            samples_classes[family] = []
                        samples_classes[family].append(sample)

                        sample.add_features(features)

                        sample.string_label = string_label
                        sample.label = label
                        sample.family = family

                        m += 1
                else:
                    string_label = 'benign'
                    label = -1 # -1 for benign

                    if b < self.benign_max: 
                        sample = Sample(hash, self.possible_features) 
                        samples[hash] = sample
                        samples_classes[string_label].append(sample)

                        sample.add_features(features)

                        sample.string_label = string_label
                        sample.label = label
                        sample.family = 'benign'

                        b += 1

        print(f'Benign Samples: {b}')
        print(f'Malware Samples: {m}')
        
        return samples, samples_classes

    def _possible_features(self) -> set:
        '''
        Returns a set of all possible features
        '''
        possible_features = set()

        with zipfile.ZipFile(self.feature_vectors_file, 'r') as zip:
            for feature_file in zip.namelist(): # iterate over all file names
                with zip.open(feature_file) as ff: # read the files
                    content = ff.read()
                    features = content.strip().split(b'\n') # get each line into a list

                    for feature in features:
                        if feature:
                            possible_features.add(feature.strip())
        
        print(f'Possible features: {len(possible_features)}')
        return possible_features
    
    def _families(self):
        '''
        returns a dict {sample_hash:family} (str:str)
        '''
        df = pd.read_csv(self.families_file, header=0) # use first row as a header
        return dict(zip(df['sha256'], df['family']))
    
    def ordered_lists(self, samples = [], chunk_size = 2500):
        '''
        Calculates and returns feature and label lists.\n
        Sample features only show features for that sample, this adds the 0s.\n
        @returns:\n
        \tfeature_list [[features] for all samples], label_list [label for all samples]
        '''
        if not samples:
            samples = list(self.samples.values())

        num_samples = len(samples)
        num_features = len(self.possible_features)

        try:
            feature_array = np.zeros((num_samples, num_features), dtype=np.uint8)
            label_array = np.zeros(num_samples, dtype=np.int8)
        except MemoryError as e:
            print(f'{e}.\n Change sample count!')
            exit()
        
        # Convert possible_features to list once for indexing, since sets don't guarantee order
        features_list = list(self.possible_features)
        
        # Create dict for faster feature lookup
        feature_to_idx = {feature: idx for idx, feature in enumerate(features_list)}
        
        # go over the chunks
        for start in range(0, num_samples, chunk_size):

            # clamp if greater than sample size
            end = min(start + chunk_size, num_samples)
            current_chunk_size = end - start

            # uses less memory
            feature_array = np.zeros((current_chunk_size, num_features), dtype=np.uint8)
            label_array = np.zeros(current_chunk_size, dtype=np.int8)
            
            # iterate over the chunk. 
            for idx_in_chunk, sample in enumerate(samples[start:end]):

                for feature in sample.features: # if feature is present in the sample

                    if feature in feature_to_idx: # if 
                        feature_array[idx_in_chunk, feature_to_idx[feature]] = 1

                label_array[idx_in_chunk] = sample.label
            
            # yield pauses the process, allowing to return to it
            yield feature_array, label_array

    def nonzero_ordered_lists(self, samples, nonzero_features):
        '''
        Same as ordered lists but for nonzero features only
        '''

        # get indexes of nonzero features in the original feature list
        features_list = list(self.possible_features)
        feature_to_idx = {feature: idx for idx, feature in enumerate(features_list)}
        nonzero_indices = [feature_to_idx[feature] for feature in nonzero_features]
        
        nonzero_indices = np.array(nonzero_indices)

        for feature_array, label_array in self.ordered_lists(samples):
            # select only the columns corresponding to nonzero features
            reduced_array = feature_array[:, nonzero_indices]
            yield reduced_array, label_array

    def prepare_data(self, target_class_positive = 'malware', target_class_negative = 'benign', training_percentage = 0.8, process_id = '') -> tuple:
        '''
        Assign classes and split the data for training. \n
        The method first reads the sample's string label, then the family. If either value equals a specified target class, the sample's int value is changed.\n
        @param:\n
        \ttarget_class_positive - str, target class with value 1\n
        \ttarget_class_negative - str, target class with value -1\n
        \ttraining_percentage - float, percentage of samples to be used for training\n
        \tprocess_id - str, in case this method is called in in a thread, assignes this id to the labels to avoid conflicts. Can be accessed in sample.id_labels dict.\n
        @returns:\n
        \ttuple of two lists - training and testing samples
        '''

        if not (target_class_negative or target_class_positive):
            return None
    
        positive_samples = []
        negative_samples = []

        # One-vs-All (target_class_positive provided, target_class_negative is None)
        if target_class_positive and not target_class_negative:
            # add all samples of the target class as positive
            if target_class_positive in self.samples_classes:
                positive_samples = self.samples_classes.get(target_class_positive, [])
            
            # all other classes become negative samples
            for family in self.possible_families_list:
                if family != target_class_positive:  # everything that is not the positive class
                    negative_samples += self.samples_classes.get(family, [])
        
        # All-vs-One (this is not needed and is here for consistency)
        elif target_class_negative and not target_class_positive:
            if target_class_negative in self.samples_classes:
                negative_samples = self.samples_classes.get(target_class_negative, [])
            
            for family in self.possible_families_list:
                if family != target_class_negative:
                    positive_samples += self.samples_classes.get(family, [])
        
        # One-vs-One (both target classes are provided)
        else:
            if target_class_positive in self.samples_classes:
                positive_samples = self.samples_classes.get(target_class_positive, [])
            if target_class_negative in self.samples_classes:
                negative_samples = self.samples_classes.get(target_class_negative, [])
        
        # check if we have samples for both
        if not (positive_samples and negative_samples):
            return None
        
        print(f'\t{target_class_positive if target_class_positive else 'All'} samples: {len(positive_samples)}')
        print(f'\t{target_class_negative if target_class_negative else 'All'} samples: {len(negative_samples)}')

        # assign labels to samples based on their class
        # this reassigns the labels and breaks any potential threding
        # use the id_label to avoid conflicts in case this method is called in a thread
        for sample in (positive_samples + negative_samples):
            # OvO
            if target_class_positive and target_class_negative:
                if sample.string_label == target_class_positive or sample.family == target_class_positive:
                    sample.label = 1
                    sample.set_id_label(1, process_id)
                elif sample.string_label == target_class_negative or sample.family == target_class_negative:
                    sample.label = -1
                    sample.set_id_label(-1, process_id)
            
            # OvA
            elif target_class_positive and not target_class_negative:
                if sample.string_label == target_class_positive or sample.family == target_class_positive:
                    sample.label = 1
                    sample.set_id_label(1, process_id)
                else:
                    sample.label = -1
                    sample.set_id_label(-1, process_id)
            
            # AvO
            elif not target_class_positive and target_class_negative:
                if sample.string_label == target_class_negative or sample.family == target_class_negative:
                    sample.label = -1
                    sample.set_id_label(-1, process_id)
                else:
                    sample.label = 1
                    sample.set_id_label(1, process_id)

        # split the data
        training_positive = positive_samples[:int(len(positive_samples) * training_percentage)]
        testing_positive = positive_samples[int(len(positive_samples) * training_percentage):]
        training_negative = negative_samples[:int(len(negative_samples) * training_percentage)]
        testing_negative = negative_samples[int(len(negative_samples) * training_percentage):]

        return training_positive, testing_positive, training_negative, testing_negative

class Sample:
    '''
    Class for storing samples
    '''

    def __init__(self, hash, possible_features) -> None:
        self.hash = hash
        
        # only get the features that are present in the sample. Add the 0s before making the X and Y lists
        self.features = {}

        self.family = None
        self.string_label = None
        self.label = 0 # 0 for unknown
        self.id_labels = {} # dict for storing labels with ids. See Storage.prepare_data().
         
    def add_features(self, features: list):
        for feature in features:
            if feature:
                self.features[feature] = 1

    def set_id_label(self, label, process_id):
        self.id_labels[process_id] = label

    def __hash__(self):
        # __hash__ must return an int.
        return hash(self.hash)

class _Algorithm:
    '''
    An algorithm for binary classification.\n
    @params:\n
    \ttarget_class_positive - str, target class with value 1\n
    \ttarget_class_negative - str, target class with value -1\n
    \ttraining_percentage - float, percentage of samples to be used for training\n
    '''

    def __init__(self, storage: Storage, target_class_positive = 'malware', target_class_negative = 'benign', training_percentage = 0.8, **kwargs) -> None:
        self.name = f'({target_class_positive}_{target_class_negative})'

        self.storage = storage
        self.samples = storage.samples
        self.possible_features = storage.possible_features
        self.families = storage.families

        self.target_class_positive = target_class_positive
        self.target_class_negative = target_class_negative
        
        self.data = storage.prepare_data(target_class_positive, target_class_negative, training_percentage)
        self.training_positive, self.testing_positive, self.training_negative, self.testing_negative = self.data if self.data else ([], [], [], [])
        
        
        # We store anything that we could ever want to access
        self.training_samples = self.training_positive + self.training_negative
        self.testing_samples = self.testing_positive + self.testing_negative

    def train():
        '''
        Train the model
        '''
        raise NotImplementedError()
    
    def evaluate():
        '''
        Predicts a single sample (raw prediction value)
        '''
        raise NotImplementedError()
    
    def classify():
        '''
        Classifies a single sample
        '''
        raise NotImplementedError()
    
    def test():
        '''
        Tests the testing samples and gets the accuracy
        '''
        raise NotImplementedError()
    
    def save(self, path):
        with open(path, 'wb') as f:
            # pickle can't save lambdas, and ordered_method doesn't matter after training
            self.order_method = self.storage.ordered_lists
            pickle.dump(self.__dict__, f)

    @classmethod
    def load(cls, path):
        storage = cls.__new__(cls)
        with open(path, 'rb') as f:
            storage.__dict__ = pickle.load(f)
        return storage

class SVM(_Algorithm):
    '''
    Support Vector Machine Algorithm\n
    \ttarget_class_positive - str, target class with value 1\n
    \ttarget_class_negative - str, target class with value -1\n
    \ttraining_percentage - float, percentage of samples to be used for training\n
    '''
    def __init__(self, *args, **kwargs) -> None:
        super(SVM, self).__init__(*args, **kwargs)
        self.name = 'Support Vector Machine' + f' {self.name}'
        self.backup_kwargs = kwargs
        self.order_method = self.storage.ordered_lists
        self.train(**kwargs)
        # retrain the model using only non-zero features
        self._retrain(**kwargs)

    def __hash__(self):
        return hash((self.name, self.target_class_positive, self.target_class_negative, self.training_samples, self.testing_samples))

    def get_tensors(self, X: list, Y: list):
        '''
        Returns tuple - X_tensor, Y_tensor
        '''

        X_tensor = torch.tensor(X, dtype=torch.float32)
        Y_tensor = torch.tensor(Y, dtype=torch.float32)

        return X_tensor, Y_tensor
    
    def train(self, **kwargs):
        '''
        Train model utilizing Stochastic Gradient Descent with Hinge Loss
        loss = max(0, 1 - t * y_pred)
        '''

        used_features = kwargs.get('features', self.storage.possible_features) 
        num_features = len(used_features)

        # set weight and bias to zero
        self.weight = torch.zeros(num_features, dtype=torch.float32, requires_grad=True)
        self.bias = torch.zeros(1, dtype=torch.float32, requires_grad=True)

        if not self.data:
            print(f'Training {self.name} failed: No data.')
            return

        epochs = kwargs.get('e', 5) # Number of times model goes through files
        C = kwargs.get('c', 0.0001) # Regularization parameter

        learning_rate = kwargs.get('lr', 0.001) # Size of step model takes when adjusting weights and bias
        learning_rate_method = kwargs.get('lr_method', 'exponential') # Learning rate method
        # constant, step, exponential

        if learning_rate_method == 'step':
            step_size = kwargs.get('step_size', 2)  # number of epochs between decays
            gamma = kwargs.get('decay', 0.5) # decay factor

        if learning_rate_method == 'exponential':
            gamma = kwargs.get('decay', 0.4) # decay factor
            initial_learning_rate = learning_rate

        early_stop = kwargs.get('early_stop', True) # stop training if loss doesn't improve

        if early_stop:
            min_epochs = kwargs.get('min_epochs', 7)  # number of epochs that must be performed
            wait_count = kwargs.get('wait_count', 4)  # number of epochs to check for improvement
            delta = kwargs.get('delta', 0.0005) # minimum improvement to continue training
            best_loss = float('inf')
            no_improvement = 0

        print('\tTraining...')
        for epoch in range(epochs):

            # if it's time, apply decay
            if learning_rate_method == 'step' and (not epoch % step_size):
                learning_rate = learning_rate * gamma

            if learning_rate_method == 'exponential':
                learning_rate = initial_learning_rate * (gamma ** epoch)

            # At the start of each epoch, set the total loss to zero
            total_loss = 0.0
            total_samples = 0
            
            # Process each chunk yielded by the ordered lists generator
            for feature_array, label_array in self.order_method(self.training_samples):

                X_chunk, Y_chunk = self.get_tensors(feature_array, label_array)
                chunk_size_curr = X_chunk.shape[0]
                total_samples += chunk_size_curr

                # Process each sample in the chunk
                for i in range(chunk_size_curr):

                    # Ensure tensor size matches features
                    if feature_array.shape[1] != self.weight.size(0):
                        
                        new_weight = torch.zeros(feature_array.shape[1], dtype=torch.float32, requires_grad=True)
                        
                        # Which array is smaller?
                        min_size = min(self.weight.size(0), feature_array.shape[1])

                        # Copy over weights for matching indices
                        with torch.no_grad():
                            new_weight[:min_size].copy_(self.weight[:min_size])
                        self.weight = new_weight

                    x_i = X_chunk[i] # Features of the file
                    y_i = Y_chunk[i] # Label (benign or malicious)
                    
                    # Compute the first prediction: positive if likely malware and negative if likely benign
                    y_pred = torch.dot(self.weight, x_i) + self.bias
                    
                    # max(0, 1 - t * y_pred). - Hinge loss - >= to 1, prediction is correct and confident
                    hinge_loss = torch.clamp(1 - y_i * y_pred, min=0)
                    
                    # Add regularization term to stop weights from becoming too large
                    loss = hinge_loss + C * torch.norm(self.weight)**2
                    
                    # Auto calulcate changes in w and b for loss reducation
                    loss.backward()
                    
                    # Update parameters using SGD and stops pytorch from auto updating
                    # Weights added 
                    with torch.no_grad():
                        self.weight -= learning_rate * self.weight.grad
                        self.bias -= learning_rate * self.bias.grad
                        
                    # Clear out the gradients to recompute next loop
                    self.weight.grad.zero_()
                    self.bias.grad.zero_()
                    
                    # Track total loss
                    total_loss += loss.item()

            avg_loss = total_loss / total_samples if total_samples else 0
            print(f'\t\tEpoch {epoch} average loss: {avg_loss:.4f}')
                
            # Delete the chunk tensors to free memory
            del X_chunk, Y_chunk

            # stop early if no improvements
            if early_stop and epoch > min_epochs:
                if avg_loss < best_loss - delta:
                    best_loss = avg_loss
                    no_improvement = 0
                else:
                    no_improvement += 1

                if no_improvement > wait_count:
                    print(f'\t\tBreaking at epoch {epoch}')
                    break

        print(f'\tTraining {self.name} complete.\n')
    
    def _retrain(self, **kwargs):
        '''
        Retrain the model using only non-zero features
        '''

        print(f'Retraining the model using only non-zero features...')

        # get non-zero features
        self._calculate_nonzero_features()

        if not self.nonzero_features:
            print('All features have non-zero weights, skipping retraining.')
            return
        print(f'{len(self.nonzero_features)} non-zero features out of {len(self.storage.possible_features)}')

        self.backup_kwargs['features'] = self.nonzero_features
    
        # very patchy method, sorry
        self.order_method = lambda samples, chunk_size=2500: self.storage.nonzero_ordered_lists(samples, self.nonzero_features)

        # retrain the model using only non-zero features
        self.train(**self.backup_kwargs)

    def evaluate(self, sample):
        '''
        Evaluate a single sample and return the prediction.
        '''

        if not self.data:
            print(f'{self.name} was not trained.')
            return None

        # get the cropped features if they exist
        used_features = self.backup_kwargs.get('features', self.storage.possible_features) 
        num_features = len(used_features)

        # Create a tensor for the sample features with the same order as in possible_features
        x = torch.zeros(num_features, dtype=torch.float32)
        
        # need to resize idx for the new feature set
        if used_features == self.storage.possible_features:
            # if using all features, use the original storage index
            for feature in sample.features:
                if feature in self.storage.idx:
                    x[self.storage.idx[feature]] = 1
        else:
            # if using a subset, create a new mapping
            used_features_idx = {feature: idx for idx, feature in enumerate(used_features)}
            for feature in sample.features:
                if feature in used_features_idx:
                    x[used_features_idx[feature]] = 1
        
        # Compute the prediction
        y_pred = torch.dot(self.weight, x) - self.bias
        
        return y_pred
    
    def classify(self, sample):
        '''
        Classify a sample and return an int label
        '''
        return 1 if self.evaluate(sample) >= 0 else -1
    
    def classify_string(self, sample):
        '''
        Classify a sample and return a string label
        '''
        if not self.target_class_negative:
            self.target_class_negative = 'all'
        if not self.target_class_positive:
            self.target_class_positive = 'all'
        return self.target_class_positive if self.classify(sample) == 1 else self.target_class_negative

    def test(self):
        correct = 0
        wrong = 0

        for sample in self.testing_samples:
            classification = self.classify(sample)
            if classification == sample.label:
                correct += 1
            else:
                wrong += 1

        if not correct + wrong:
            return 0

        accuracy = correct / (correct + wrong)
        print(f'{self.name} - Accuracy: {accuracy}')
        
        return accuracy
    
    def _calculate_nonzero_features(self):
        '''
        Returns a dict {weight:feature}
        '''
        weights = self.weight.detach().numpy()
        abs_weights = np.abs(weights)
        non_zero_indices = np.where(abs_weights > 0)[0]

        features_list = list(self.storage.possible_features)
        self.nonzero_features = [features_list[i] for i in non_zero_indices]

    def show_feature_importance(self, top_n=10):
        '''
        Shows the most and least important features based on weight
        '''
        
        # convert weight tensor to numpy and get absolute values
        weights = self.weight.detach().numpy()
        abs_weights = np.abs(weights)
            
        # get the top features
        sorted_indices_most = np.argsort(abs_weights)[-top_n:][::-1]  # reverse to get the top
        
        # get non-zero features for least important
        non_zero_indices = np.where(abs_weights > 0)[0]

        # get the bottom non-zero features
        sorted_non_zero_indices = non_zero_indices[np.argsort(abs_weights[non_zero_indices])] # 
        sorted_indices_least = sorted_non_zero_indices[:top_n]  # get top n lowest non-zero features 
        
        features_list = list(self.storage.possible_features)
        
        print(f'\n{top_n} Most Important Features:')
        for idx in sorted_indices_most:
            feature = features_list[idx]
            weight = weights[idx]
            print(f'  {feature.decode('utf-8', errors='replace') if isinstance(feature, bytes) else feature}: {weight:3f}')
        
        # show least important features with non-zero weights
        actual_count = min(top_n, len(sorted_indices_least))
        print(f'\n{actual_count} Least Important Features (non-zero weights):')
        for idx in sorted_indices_least:
            feature = features_list[idx]
            weight = weights[idx]
            print(f'  {feature.decode('utf-8', errors='replace') if isinstance(feature, bytes) else feature}: {weight}')
        
        # show how many features are not used
        zero_weight_count = np.sum(abs_weights == 0)
        print(f'\nFeatures with zero weight: {zero_weight_count} out of {len(weights)}')